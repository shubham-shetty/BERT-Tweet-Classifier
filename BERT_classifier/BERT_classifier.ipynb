{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CS685_HW1_annotation_task_final.ipynb","provenance":[{"file_id":"1cTDpMUuJbxxaGEcNe11KQXLLweu8OA8r","timestamp":1635299408073},{"file_id":"1CRZfedkwWd5_NAIVp1vo6bz5xYkyTFgC","timestamp":1634149359597},{"file_id":"1K9H753cX0tD0lsoXvyHsDhrTtbnzq1bL","timestamp":1603002079306},{"file_id":"18v_7cFNT362Lzcmyg_PNKVnGQgVKc3i2","timestamp":1602459467767},{"file_id":"1bcAXWjkz8V8PK1FhnBrsb5YZAHKJiVFe","timestamp":1601580687210},{"file_id":"1wgo33YMqyTmwPXBCgDYvDD39Hgz516zV","timestamp":1599667757648},{"file_id":"1ZNQQshRjVp-0vLNi6ZGRtXX102EWHRq4","timestamp":1598302241860},{"file_id":"1XOa--UHuAQpBRcdqYbFcb8QuUTvywsSk","timestamp":1568522504552},{"file_id":"1LShMg_-e2SzrjDMxSgVbnYyTAgwcJov0","timestamp":1568420694683}],"collapsed_sections":["SgNZTjrhcHa0"]},"kernelspec":{"display_name":"Python 3","name":"python3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"SgNZTjrhcHa0"},"source":["## Homework 1, CS685 Fall 2021\n","\n","### This is due on November 5th, 2021. This notebook is to be submitted via Gradescope as a PDF file, while your three dataset files (annotator1.csv, annotator2.csv, and final_data.csv) should be emailed to cs685instructors@gmail.com with the subject line formatted as **Firstname_Lastname_HW1data**. 100 points total.\n","\n","#### IMPORTANT: After copying this notebook to your Google Drive, please paste a link to it below. To get a publicly-accessible link, hit the *Share* button at the top right, then click \"Get shareable link\" and copy over the result. If you fail to do this, you will receive no credit for this homework!\n","***LINK:*** https://colab.research.google.com/drive/1ioBEoNZxk93p-mpUiDAjJH7ZnR6ZJPer?usp=sharing\n","\n","---\n","\n","\n","##### *How to submit this problem set:*\n","- Write all the answers in this Colab notebook. Once you are finished, generate a PDF via (File -> Print -> Save as PDF) and upload it to Gradescope.\n","  \n","- **Important:** check your PDF before you submit to Gradescope to make sure it exported correctly. If Colab gets confused about your syntax, it will sometimes terminate the PDF creation routine early.\n","\n","- **Important:** on Gradescope, please make sure that you tag each page with the corresponding question(s). This makes it significantly easier for our graders to grade submissions, especially with the long outputs of many of these cells. We will take off points for submissions that are not tagged.\n","\n","- When creating your final version of the PDF to hand in, please do a fresh restart and execute every cell in order. One handy way to do this is by clicking `Runtime -> Run All` in the notebook menu.\n","\n","---\n","\n","##### *Academic honesty*\n","\n","- We will audit the Colab notebooks from a set number of students, chosen at random. The audits will check that the code you wrote actually generates the answers in your PDF. If you turn in correct answers on your PDF without code that actually generates those answers, we will consider this a serious case of cheating. See the course page for honesty policies.\n","\n","- We will also run automatic checks of Colab notebooks for plagiarism. Copying code from others is also considered a serious case of cheating.\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"r2dYC4HbZL-j"},"source":["# Part 1: Annotation\n","\n","In this homework, you will first collect a labeled dataset of **120** sentences for a text classification task of your choice. This process will include:\n","\n","1. *Data collection*: Collect 120 sentences from any source you find interesting (e.g., literature, Tweets, news articles, reviews, etc.)\n","\n","2. *Task design*: Come up with a binary (i.e., only two labels) sentence-level classification task that you would like to perform on your sentences. Don't choose something boring like sentiment analysis... Be creative! Write up annotator guidelines/instructions on how you would like people to label your data.\n","\n","3. On your dataset, collect annotations from **two** classmates for your task. Everyone in this class will need to both create their own dataset and also serve as an annotator for two other classmates. In order to get everything done on time, you need to complete the following steps:\n","\n","> *   Find two classmates willing to label 120 sentences each (we will open a Piazza thread to help facilitate this).\n","*   Send them your annotation guidelines and a way that they can easily annotate the data (e.g., a spreadsheet or Google form)\n","*   Collect the labeled data from each of the two annotators.\n","*   Sanity check the data for basic cleanliness (are all examples annotated? are all labels allowable ones?)\n","\n","4. Collect feedback from annotators about the task including annotation time and obstacles encountered (e.g., maybe your guidelines were confusing! or maybe some sentences were particularly hard to annotate!)\n","\n","5. Calculate and report inter-annotator agreement.\n","\n","6. Aggregate output from both annotators to create final dataset.\n","\n","7. Perform NLP experiments on your new dataset!"]},{"cell_type":"markdown","metadata":{"id":"Heui1z3IjZh_"},"source":["## Question 1.1 (10 points):\n","Describe the source of your unlabeled data, why you chose it, and what kind of sentence selection process you used (if any) to choose 120 sentences for annotation. Also briefly describe the text classification task that you will be collecting labels for in the next section.\n","\n","### *WRITE YOUR ANSWER HERE* ###\n","#### Dataset Description: ####\n","Gender Classification for Political Tweets  \n","\n","#### Dataset Source: ####\n","I pulled my text data from Twitter using a tweet scraping code written by me (https://github.com/shubham-shetty/twitter-scraper). I pulled the 20 latest tweets from multiple political figures, journalists and commentators and manually selected 120 relevant tweets from this corpus.  \n","\n","#### Task & Motivation: ####\n","Ideally, politics should see equal representation from both sexes. However, American politics is seen to be largely male-dominated. Political communications between genders may have differences due to this gender-salient context (Hu & Kearney, 2020). These communication differences may also manifest in electronic communication such as on Twitter. It is important that the role of the marginalized sex in American politics has an equal voice on such a platform, and the issues they raise have as much weightage as anything else. Analyzing political tweets and classifying them as Male or Female can help in ensuring that Twitter as a platform provides equal opportunity for heterogeneous voices to be heard, and does not act as a homogeneous echo chamber."]},{"cell_type":"markdown","metadata":{"id":"EynpTLydj7IM"},"source":["## Question 1.2 (25 points):\n","Copy the annotation guidelines that you provided to your classmates below. We expect that these guidelines will be very detailed (and as such could be fairly long). You must include:\n","\n","> *   The two categories for your binary classification problem, including the exact strings you want annotators to use while labeling. \n","*   Descriptions of the categories and what they mean.\n","*   Representative examples of each category (i.e., sentences from outside your dataset that you have manually labeled to give annotators an idea of how to perform the task)\n","*   A discussion of of tricky corner cases, and criteria to help the annotator decide them. If you look at the data and think about how an annotator could do the task, you will likely find a bunch of these!\n","\n","\n","---\n","\n","\n","\n","\n","### *COPY YOUR ANNOTATION GUIDELINES HERE.* Please format them nicely so it's easy for us to read / grade :)### \n","\n","1. Tweets will be labeled as either 'M' for male or 'F' for female.  \n","\n","2. Tweets dealing with topics which are largely considered to be associated with men can be labelled as 'M', for instance defense, gun control, sports etc. Examples -   \n","    a. Democrats have taken our roads, bridges, ports, airports, and waterways hostage to ram through an historically reckless taxing and spending spree that would hurt families and help China. The far left is running Congress and the American people are hurting. (@LeaderMcConnell)  \n","    b. Why are people worried about giving IRS more information? By a 72% to 14% margin, voters think some agents or political leaders would use additional information to harm political opponents. (@ScottWRasmussen)\n","\n","3.  Tweets dealing with topics which are largely considered to be associated with women can be labelled as 'F', for instance - reproductive rights, family law, female representation etc. Examples -  \n","    a. Happy International #DayoftheGirl! Today I’m celebrating the promise that exists within every girl. When we invest in that promise, we empower them to change their communities for the better. Join me and the @GirlsAlliance in supporting girls’ education (@MichelleObama)  \n","    b. This is the day I learned my Pelosi bio, MADAM SPEAKER, has sold ZERO copies in Wichita, where I was born and raised. So I ask my brother....my classmates from Kos Harris Elementary &amp; Robinson Jr HS &amp; Southeast HS: Really? Does @Watermarkbooks even stock it? #HeartlandHeartbreak (@SusanPage)  \n","\n","4. More ambiguous tweets can be labelled based on following observations (Bamman et al., 2014)-  \n","    a. Females tend to use more of the following text markers - pronouns, emotion terms, CMC words (lol, omg), emoticons, hesitation words (err, umm)  \n","    b.  Males tend to use more of the following text markers - numbers, technology words, swear words.  \n","    c.  Following markers have mixed or negligible usage - family terms, assent, negation, articles, quantifiers.  \n","\n","5. Trust your instincts  \n"]},{"cell_type":"markdown","metadata":{"id":"935duWappc-o"},"source":["## Question 1.3 (5 points):\n","Write down the names and emails of the two classmates who will be annotating your data below. Once they are finished annotating, create two .csv files (annotator1.csv and annotator2.csv) that contains each annotator's labels. The file should have two columns with headers **text** and **label**, respectively. You will include these files in an email to the instructors account when you're finished with this homework. \n","\n","*The tweets.csv file provided as an example in Part 2 below uses the same format.*\n","\n","### *WRITE CLASSMATE 1 NAME/EMAIL HERE:* ###   \n","Adam Viola (aviola@umass.edu)\n","\n","### *WRITE CLASSMATE 2 NAME/EMAIL HERE:* ###   \n","Pranjali Parse (pparse@umass.edu)"]},{"cell_type":"markdown","metadata":{"id":"bcIMegO_uPRK"},"source":["## Question 1.4 (10 points):\n","After both annotators have finished labeling the 120 sentences you gave them, ask them for feedback about your task and the provided annotation guidelines. If you were to collect more labeled data for this task in the future, what would you change from your current setup? Why? Please include a summary of annotator feedback (with specific examples that they found challenging to label) in your answer.\n","\n","### *WRITE ANSWER HERE* ###  \n","Following points were raised through feedback from the annotators -  \n","\n","1. Too many tweets were ambiguous.\n","2. Annotators were vary of bias creeping into their annotations.\n","3. Is it actually possible to just bifurcate tweets into male and female? How would it handle the case when there are more than two genders?\n","\n","\n","Some changes which could be made if this annotation task was undertaken at a larger scale, keeping these feedback in mind, could be -   \n","\n","1. Needs more clearer guidelines for classifying tweets as male or female. More research could be done to understand female-centric and male-centric topics in the political sphere. \n","2. Instead of providing just 2 different classes to label, maybe we could provide a set of attributes on which annotators could rate tweets on a scale (like 1-5, 1 for least likely to 5 for most likely)."]},{"cell_type":"markdown","metadata":{"id":"4ltte0Z-vD0u"},"source":["## Question 1.5 (10 points):\n","Now, compute the inter-annotator agreement between your two annotators. Upload both .csv files to your Colab session (click the folder icon in the sidebar to the left of the screen). In the code cell below, read the data from the two files and compute both the raw agreement (% of examples for which both annotators agreed on the label) and the [Cohen's Kappa](https://en.wikipedia.org/wiki/Cohen%27s_kappa). Feel free to use implementations in existing libraries (e.g., [sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.cohen_kappa_score.html)). After you're done, paste the numbers in the text cell that follows your code. \n","\n","*If you're curious, Cohen suggested the Kappa result be interpreted as follows: values ≤ 0 as indicating no agreement and 0.01–0.20 as none to slight, 0.21–0.40 as fair, 0.41– 0.60 as moderate, 0.61–0.80 as substantial, and 0.81–1.00 as almost perfect agreement.*"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lm_JMtpQiHHL","executionInfo":{"status":"ok","timestamp":1636155708071,"user_tz":240,"elapsed":20811,"user":{"displayName":"Shubham Shetty","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gift8m3G6SXjNLimI9-jKJTPDMNVC-Cto60CKMc=s64","userId":"02610804697838200174"}},"outputId":"a2d7fa25-c6bb-4c4b-e748-4cc79e34c5e9"},"source":["# This mounts your Google Drive to the Colab VM.\n","from google.colab import drive\n","drive.mount('/content/gdrive', force_remount=True)\n","\n","# Define folder for annotated data\n","FOLDERNAME = 'Coursework/Fall 2021/685/Assignments/annotated_data'\n","assert FOLDERNAME is not None, \"[!] Enter the foldername.\"\n","\n","# Now that we've mounted your Drive, this ensures that\n","# the Python interpreter of the Colab VM can load\n","# python files from within it.\n","import sys\n","sys.path.append('/content/gdrive/My Drive/{}'.format(FOLDERNAME))\n","\n","#Switch to working directory\n","%cd /content/gdrive/My\\ Drive/$FOLDERNAME"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n","/content/gdrive/My Drive/Coursework/Fall 2021/685/Assignments/annotated_data\n"]}]},{"cell_type":"code","metadata":{"id":"UD7yvkwgy82S","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1636155709101,"user_tz":240,"elapsed":1038,"user":{"displayName":"Shubham Shetty","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gift8m3G6SXjNLimI9-jKJTPDMNVC-Cto60CKMc=s64","userId":"02610804697838200174"}},"outputId":"bf94b832-3bdf-424b-dfb9-8b78eaa9e9ec"},"source":["### WRITE CODE TO LOAD ANNOTATIONS AND \n","### COMPUTE AGREEMENT + COHEN'S KAPPA HERE!\n","import pandas as pd\n","from sklearn import metrics as skl\n","\n","# Load annotations\n","annotator1 = pd.read_csv('annotator1.csv', header=0, names=['row_num','text','gender'], usecols = ['text','gender'])\n","annotator2 = pd.read_csv('annotator2.csv', header=0, names=['row_num','text','gender'], usecols = ['text','gender'])\n","\n","annotations1 = annotator1['gender'].to_numpy()\n","annotations2 = annotator2['gender'].to_numpy()\n","\n","# Calculate Raw Agreement\n","match = 0\n","N = len(annotations1)\n","for i in range(N):\n","  if annotations1[i] == annotations2[i]:\n","    match +=1\n","raw_agreement = (match/N)*100\n","print(f'Raw Agreement = {raw_agreement}')\n","\n","# Calculate Cohen's Kappa\n","kappa = skl.cohen_kappa_score(annotations1, annotations2, labels=['M', 'F'])\n","print(f'Cohen\\'s Kappa = {kappa}')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Raw Agreement = 68.33333333333333\n","Cohen's Kappa = 0.34104046242774566\n"]}]},{"cell_type":"markdown","metadata":{"id":"Qd7Xq4SKzF5U"},"source":["###*RAW AGREEMENT*: 68.33 %\n","###*COHEN'S KAPPA*: 0.34104"]},{"cell_type":"markdown","metadata":{"id":"t2-1LkRHze5N"},"source":["## Question 1.6 (10 points):\n","To form your final dataset, you need to *aggregate* the annotations from both annotators (i.e., for cases where they disagree, you need to choose a single label). Use any method you like other than random label selection to perform this aggregation (e.g., have the two annotators discuss each disagreement and come to consensus, or choose the label you agree with the most). Upload your final dataset to the Colab session (in the same format as the other two files) as final_dataset.csv. Remember to include this file in your final email to us!\n","\n","### *DESCRIBE YOUR AGGREGATION STRATEGY HERE* ###\n","I used a third annotated dataset for tie-breaks. This dataset was annotated by another friend. The strategy used for tie-breaks was -  \n","\n","1. If annotator 1 and 2 are in agreement, keep the annotated class.\n","2. Else, use the class as annotated in the tie-break data."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AvE6yZS3vvsG","executionInfo":{"status":"ok","timestamp":1636155709261,"user_tz":240,"elapsed":165,"user":{"displayName":"Shubham Shetty","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gift8m3G6SXjNLimI9-jKJTPDMNVC-Cto60CKMc=s64","userId":"02610804697838200174"}},"outputId":"8f0b1a0a-7c4b-4d26-8551-d0feefc2f5c8"},"source":["final_dataset = pd.read_csv('final_dataset.csv', header=0, names=['row_num','text','gender'], usecols = ['text','gender'])\n","print(final_dataset)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["                                                  text gender\n","0    Does seem like a tell that whenever Youngkin g...      M\n","1    @LisaDTRice Everyone is entitled to voice thei...      F\n","2    I’m so encouraged that the @POTUS and @VP have...      F\n","3    Democrats fear for their reelections because o...      F\n","4    It's SHOCKING to think that the Patriot Act is...      F\n","..                                                 ...    ...\n","115  Anyways this one goes out to anyone in any wor...      F\n","116  Democrats want to pay for their reckless spend...      M\n","117  Amazon's internal systems sound like a nightma...      M\n","118  I actually would be surprised if Trump went to...      M\n","119                      Three words: tax the wealthy.      M\n","\n","[120 rows x 2 columns]\n"]}]},{"cell_type":"markdown","metadata":{"id":"d23zfO_ALKeB"},"source":["# Part 2: Text classification"]},{"cell_type":"markdown","metadata":{"id":"N25dvF4jvYoy"},"source":["Now we'll move onto fine-tuning  pretrained language models specifically on your dataset. This part of the homework is meant to be an introduction to the HuggingFace library, and it contains code that will potentially be useful for your final projects. Since we're dealing with large models, the first step is to change to a GPU runtime.\n","\n","## Adding a hardware accelerator\n","\n","Please go to the menu and add a GPU as follows:\n","\n","`Edit > Notebook Settings > Hardware accelerator > (GPU)`\n","\n","Run the following cell to confirm that the GPU is detected."]},{"cell_type":"code","metadata":{"id":"edOh9ooiIW1B","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1636155734354,"user_tz":240,"elapsed":25102,"user":{"displayName":"Shubham Shetty","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gift8m3G6SXjNLimI9-jKJTPDMNVC-Cto60CKMc=s64","userId":"02610804697838200174"}},"outputId":"3dd9e504-3974-4cf3-fd19-7e96c0127d2a"},"source":["import torch\n","\n","# Confirm that the GPU is detected\n","\n","assert torch.cuda.is_available()\n","\n","# Get the GPU device name.\n","device_name = torch.cuda.get_device_name()\n","n_gpu = torch.cuda.device_count()\n","print(f\"Found device: {device_name}, n_gpu: {n_gpu}\")\n","device = torch.device(\"cuda\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found device: Tesla K80, n_gpu: 1\n"]}]},{"cell_type":"markdown","metadata":{"id":"xrvH7xx9LnMC"},"source":["## Installing Hugging Face's Transformers library\n","We will use Hugging Face's Transformers (https://github.com/huggingface/transformers), an open-source library that provides general-purpose architectures for natural language understanding and generation with a collection of various pretrained models made by the NLP community. This library will allow us to easily use pretrained models like `BERT` and perform experiments on top of them. We can use these models to solve downstream target tasks, such as text classification, question answering, and sequence labeling.\n","\n","Run the following cell to install Hugging Face's Transformers library and download a sample data file called tweets.csv that contains tweets about airlines along with a negative, neutral, or positive sentiment rating. Note that you will be asked to link with your Google Drive account to download some of these files. If you're concerned about security risks (there have not been any issues in previous semesters), feel free to make a new Google account and use it for this homework!"]},{"cell_type":"code","metadata":{"id":"gtqS2e5fxpqa","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1636155763897,"user_tz":240,"elapsed":29592,"user":{"displayName":"Shubham Shetty","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gift8m3G6SXjNLimI9-jKJTPDMNVC-Cto60CKMc=s64","userId":"02610804697838200174"}},"outputId":"1f167e46-e925-4e79-fae5-74a7b1d5b084"},"source":["!pip install transformers\n","!pip install -U -q PyDrive\n","\n","from pydrive.auth import GoogleAuth\n","from pydrive.drive import GoogleDrive\n","from google.colab import auth\n","from oauth2client.client import GoogleCredentials\n","# Authenticate and create the PyDrive client.\n","auth.authenticate_user()\n","gauth = GoogleAuth()\n","gauth.credentials = GoogleCredentials.get_application_default()\n","drive = GoogleDrive(gauth)\n","print('success!')\n","\n","import os\n","import zipfile\n","\n","# Download helper functions file\n","helper_file = drive.CreateFile({'id': '16HW-z9Y1tM3gZ_vFpJAuwUDohz91Aac-'})\n","helper_file.GetContentFile('helpers.py')\n","print('helper file downloaded! (helpers.py)')\n","\n","# Download sample file of tweets\n","data_file = drive.CreateFile({'id': '1QcoAmjOYRtsMX7njjQTYooIbJHPc6Ese'})\n","data_file.GetContentFile('tweets.csv')\n","print('sample tweets downloaded! (tweets.csv)')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting transformers\n","  Downloading transformers-4.12.3-py3-none-any.whl (3.1 MB)\n","\u001b[K     |████████████████████████████████| 3.1 MB 5.3 MB/s \n","\u001b[?25hCollecting sacremoses\n","  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n","\u001b[K     |████████████████████████████████| 895 kB 51.9 MB/s \n","\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.3.0)\n","Collecting tokenizers<0.11,>=0.10.1\n","  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n","\u001b[K     |████████████████████████████████| 3.3 MB 32.1 MB/s \n","\u001b[?25hCollecting pyyaml>=5.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 47.6 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Collecting huggingface-hub<1.0,>=0.1.0\n","  Downloading huggingface_hub-0.1.1-py3-none-any.whl (59 kB)\n","\u001b[K     |████████████████████████████████| 59 kB 7.1 MB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.7.4.3)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (2.4.7)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n","Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed huggingface-hub-0.1.1 pyyaml-6.0 sacremoses-0.0.46 tokenizers-0.10.3 transformers-4.12.3\n","success!\n","helper file downloaded! (helpers.py)\n","sample tweets downloaded! (tweets.csv)\n"]}]},{"cell_type":"markdown","metadata":{"id":"-8XIL7wPovVX"},"source":["The cell below imports some helper functions we wrote to demonstrate the task on the sample tweet dataset."]},{"cell_type":"code","metadata":{"id":"Taseb33Sovg0"},"source":["from helpers import tokenize_and_format, flat_accuracy"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gKc0xYh-MAbc"},"source":["# Part 1: Data Prep and Model Specifications\n","\n","Upload your data using the file explorer to the left. We have provided a function below to tokenize and format your data as BERT requires. Make sure that your csv file, titled final_data.csv, has one column \"text\" and another column \"labels\" containing integers.\n","\n","If you run the cell below without modifications, it will run on the tweets.csv example data we have provided. It imports some helper functions we wrote to demonstrate the task on the sample tweet dataset. You should first run all of the following cells with tweets.csv just to see how everything works. Then, once you understand the whole preprocessing / fine-tuning process, change the csv in the below cell to your final_data.csv file, add any extra preprocessing code you wish, and then run the cells again on your own data."]},{"cell_type":"code","metadata":{"id":"YGhkeLQlNNr8","colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"status":"ok","timestamp":1636163333595,"user_tz":240,"elapsed":1232,"user":{"displayName":"Shubham Shetty","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gift8m3G6SXjNLimI9-jKJTPDMNVC-Cto60CKMc=s64","userId":"02610804697838200174"}},"outputId":"43dded58-3593-47e8-cf56-dab158d48731"},"source":["# Upload final_dataset.csv before running\n","\n","from helpers import tokenize_and_format, flat_accuracy\n","import pandas as pd\n","\n","# Pre-processing\n","df = pd.read_csv('final_dataset.csv', header=0, names=['row_num','text','label'], usecols = ['text','label'])\n","df['label'].replace(['M', 'F'], [0,1], inplace=True)\n","#print(df)\n","#df = pd.read_csv('tweets.csv')\n","\n","df = df.sample(frac=1).reset_index(drop=True)\n","\n","texts = df.text.values\n","labels = df.label.values\n","\n","### tokenize_and_format() is a helper function provided in helpers.py ###\n","input_ids, attention_masks = tokenize_and_format(texts)\n","\n","# Convert the lists into tensors.\n","input_ids = torch.cat(input_ids, dim=0)\n","attention_masks = torch.cat(attention_masks, dim=0)\n","labels = torch.tensor(labels)\n","\n","# Print sentence 0, now as a list of IDs.\n","print('Original: ', texts[0])\n","print('Token IDs:', input_ids[0])"],"execution_count":104,"outputs":[{"output_type":"stream","name":"stdout","text":["Original:  Great to be with some of the brave men and women of California Law Enforcement! #BackTheBlue\n","Token IDs: tensor([  101,  2307,  2000,  2022,  2007,  2070,  1997,  1996,  9191,  2273,\n","         1998,  2308,  1997,  2662,  2375,  7285,   999,  1001,  2067, 10760,\n","        16558,  5657,   102,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0])\n"]}]},{"cell_type":"markdown","metadata":{"id":"H3D-CzQEUXYz"},"source":["## Create train/test/validation splits\n","\n","Here we split your dataset into 3 parts: a training set, a validation set, and a testing set. Each item in your dataset will be a 3-tuple containing an input_id tensor, an attention_mask tensor, and a label tensor.\n","\n"]},{"cell_type":"code","metadata":{"id":"kGgeZ3M0UWs0","executionInfo":{"status":"ok","timestamp":1636163334296,"user_tz":240,"elapsed":3,"user":{"displayName":"Shubham Shetty","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gift8m3G6SXjNLimI9-jKJTPDMNVC-Cto60CKMc=s64","userId":"02610804697838200174"}}},"source":["\n","total = len(df)\n","\n","num_train = int(total * .8)\n","num_val = int(total * .1)\n","num_test = total - num_train - num_val\n","\n","# make lists of 3-tuples (already shuffled the dataframe in cell above)\n","\n","train_set = [(input_ids[i], attention_masks[i], labels[i]) for i in range(num_train)]\n","val_set = [(input_ids[i], attention_masks[i], labels[i]) for i in range(num_train, num_val+num_train)]\n","test_set = [(input_ids[i], attention_masks[i], labels[i]) for i in range(num_val + num_train, total)]\n","\n","train_text = [texts[i] for i in range(num_train)]\n","val_text = [texts[i] for i in range(num_train, num_val+num_train)]\n","test_text = [texts[i] for i in range(num_val + num_train, total)]\n"],"execution_count":105,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QCr006iTkqwM"},"source":["Here we choose the model we want to finetune from https://huggingface.co/transformers/pretrained_models.html. Because the task requires us to label sentences, we wil be using BertForSequenceClassification below. You may see a warning that states that `some weights of the model checkpoint at [model name] were not used when initializing. . .` This warning is expected and means that you should fine-tune your pre-trained model before using it on your downstream task. See [here](https://github.com/huggingface/transformers/issues/5421#issuecomment-652582854) for more info."]},{"cell_type":"code","metadata":{"id":"lPo640_ZlEPK","colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"status":"ok","timestamp":1636163337914,"user_tz":240,"elapsed":1909,"user":{"displayName":"Shubham Shetty","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gift8m3G6SXjNLimI9-jKJTPDMNVC-Cto60CKMc=s64","userId":"02610804697838200174"}},"outputId":"e4fe6d78-8a63-4bb5-ba23-185e222adb23"},"source":["from transformers import BertForSequenceClassification, AdamW, BertConfig\n","\n","model = BertForSequenceClassification.from_pretrained(\n","    \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n","    num_labels = 2, # The number of output labels.   \n","    output_attentions = False, # Whether the model returns attentions weights.\n","    output_hidden_states = False, # Whether the model returns all hidden-states.\n",")\n","\n","# Tell pytorch to run this model on the GPU.\n","model.cuda()\n"],"execution_count":106,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"execute_result","data":{"text/plain":["BertForSequenceClassification(\n","  (bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (dropout): Dropout(p=0.1, inplace=False)\n","  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",")"]},"metadata":{},"execution_count":106}]},{"cell_type":"markdown","metadata":{"id":"i3lLdoW_le3M"},"source":["# ACTION REQUIRED #\n","\n","Define your fine-tuning hyperparameters in the cell below (we have randomly picked some values to start with). We want you to experiment with different configurations to find the one that works best (i.e., highest accuracy) on your validation set. Feel free to also change pretrained models to others available in the HuggingFace library (you'll have to modify the cell above to do this). You might find papers on BERT fine-tuning stability (e.g., [Mosbach et al., ICLR 2021](https://openreview.net/pdf?id=nzpLWnVAyah)) to be of interest."]},{"cell_type":"code","metadata":{"id":"Dd2JdC6IletV","executionInfo":{"status":"ok","timestamp":1636163337917,"user_tz":240,"elapsed":18,"user":{"displayName":"Shubham Shetty","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gift8m3G6SXjNLimI9-jKJTPDMNVC-Cto60CKMc=s64","userId":"02610804697838200174"}}},"source":["batch_size = 16\n","optimizer = AdamW(model.parameters(),\n","                  lr = 5e-5, # args.learning_rate - default is 5e-5\n","                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8\n","                )\n","epochs = 5"],"execution_count":107,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Pd4fwn_el1ge"},"source":["# Fine-tune your model\n","Here we provide code for fine-tuning your model, monitoring the loss, and checking your validation accuracy. Rerun both of the below cells when you change your hyperparameters above."]},{"cell_type":"code","metadata":{"id":"8A793_vAmtnM","executionInfo":{"status":"ok","timestamp":1636163339671,"user_tz":240,"elapsed":6,"user":{"displayName":"Shubham Shetty","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gift8m3G6SXjNLimI9-jKJTPDMNVC-Cto60CKMc=s64","userId":"02610804697838200174"}}},"source":["batch_size = 16\n","optimizer = AdamW(model.parameters(),\n","                  lr = 5e-5, # args.learning_rate - default is 5e-5\n","                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8\n","                )\n","epochs = 10"],"execution_count":108,"outputs":[]},{"cell_type":"code","metadata":{"id":"y2b38qzUm3AF","executionInfo":{"status":"ok","timestamp":1636163340720,"user_tz":240,"elapsed":315,"user":{"displayName":"Shubham Shetty","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gift8m3G6SXjNLimI9-jKJTPDMNVC-Cto60CKMc=s64","userId":"02610804697838200174"}}},"source":["import numpy as np\n","# function to get validation accuracy\n","def get_validation_performance(val_set):\n","    # Put the model in evaluation mode\n","    model.eval()\n","\n","    # Tracking variables \n","    total_eval_accuracy = 0\n","    total_eval_loss = 0\n","\n","    num_batches = int(len(val_set)/batch_size) + 1\n","\n","    total_correct = 0\n","\n","    for i in range(num_batches):\n","\n","      end_index = min(batch_size * (i+1), len(val_set))\n","\n","      batch = val_set[i*batch_size:end_index]\n","      \n","      if len(batch) == 0: continue\n","\n","      input_id_tensors = torch.stack([data[0] for data in batch])\n","      input_mask_tensors = torch.stack([data[1] for data in batch])\n","      label_tensors = torch.stack([data[2] for data in batch])\n","      \n","      # Move tensors to the GPU\n","      b_input_ids = input_id_tensors.to(device)\n","      b_input_mask = input_mask_tensors.to(device)\n","      b_labels = label_tensors.to(device)\n","        \n","      # Tell pytorch not to bother with constructing the compute graph during\n","      # the forward pass, since this is only needed for backprop (training).\n","      with torch.no_grad():        \n","\n","        # Forward pass, calculate logit predictions.\n","        outputs = model(b_input_ids, \n","                                token_type_ids=None, \n","                                attention_mask=b_input_mask,\n","                                labels=b_labels)\n","        loss = outputs.loss\n","        logits = outputs.logits\n","            \n","        # Accumulate the validation loss.\n","        total_eval_loss += loss.item()\n","        \n","        # Move logits and labels to CPU\n","        logits = logits.detach().cpu().numpy()\n","        label_ids = b_labels.to('cpu').numpy()\n","\n","        # Calculate the number of correctly labeled examples in batch\n","        pred_flat = np.argmax(logits, axis=1).flatten()\n","        labels_flat = label_ids.flatten()\n","        num_correct = np.sum(pred_flat == labels_flat)\n","        total_correct += num_correct\n","        \n","    # Report the final accuracy for this validation run.\n","    avg_val_accuracy = total_correct / len(val_set)\n","    return avg_val_accuracy\n","\n"],"execution_count":109,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"id":"Oy6QEJ-Im4wK","executionInfo":{"status":"ok","timestamp":1636163364681,"user_tz":240,"elapsed":23039,"user":{"displayName":"Shubham Shetty","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gift8m3G6SXjNLimI9-jKJTPDMNVC-Cto60CKMc=s64","userId":"02610804697838200174"}},"outputId":"d4d781bb-2203-439a-e6fe-1a50631e122c"},"source":["import random\n","\n","# training loop\n","\n","# For each epoch...\n","for epoch_i in range(0, epochs):\n","    # Perform one full pass over the training set.\n","\n","    print(\"\")\n","    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n","    print('Training...')\n","\n","    # Reset the total loss for this epoch.\n","    total_train_loss = 0\n","\n","    # Put the model into training mode.\n","    model.train()\n","\n","    # For each batch of training data...\n","    num_batches = int(len(train_set)/batch_size) + 1\n","\n","    for i in range(num_batches):\n","      end_index = min(batch_size * (i+1), len(train_set))\n","\n","      batch = train_set[i*batch_size:end_index]\n","\n","      if len(batch) == 0: continue\n","\n","      input_id_tensors = torch.stack([data[0] for data in batch])\n","      input_mask_tensors = torch.stack([data[1] for data in batch])\n","      label_tensors = torch.stack([data[2] for data in batch])\n","\n","      # Move tensors to the GPU\n","      b_input_ids = input_id_tensors.to(device)\n","      b_input_mask = input_mask_tensors.to(device)\n","      b_labels = label_tensors.to(device)\n","\n","      # Clear the previously calculated gradient\n","      model.zero_grad()        \n","\n","      # Perform a forward pass (evaluate the model on this training batch).\n","      outputs = model(b_input_ids, \n","                            token_type_ids=None, \n","                            attention_mask=b_input_mask, \n","                            labels=b_labels)\n","      loss = outputs.loss\n","      logits = outputs.logits\n","\n","      total_train_loss += loss.item()\n","\n","      # Perform a backward pass to calculate the gradients.\n","      loss.backward()\n","\n","      # Update parameters and take a step using the computed gradient.\n","      optimizer.step()\n","        \n","    # ========================================\n","    #               Validation\n","    # ========================================\n","    # After the completion of each training epoch, measure our performance on\n","    # our validation set. Implement this function in the cell above.\n","    print(f\"Total loss: {total_train_loss}\")\n","    val_acc = get_validation_performance(val_set)\n","    print(f\"Validation accuracy: {val_acc}\")\n","    \n","print(\"\")\n","print(\"Training complete!\")\n"],"execution_count":110,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","======== Epoch 1 / 10 ========\n","Training...\n","Total loss: 4.495677411556244\n","Validation accuracy: 0.6666666666666666\n","\n","======== Epoch 2 / 10 ========\n","Training...\n","Total loss: 4.009741008281708\n","Validation accuracy: 0.4166666666666667\n","\n","======== Epoch 3 / 10 ========\n","Training...\n","Total loss: 3.888410806655884\n","Validation accuracy: 0.6666666666666666\n","\n","======== Epoch 4 / 10 ========\n","Training...\n","Total loss: 3.3549228608608246\n","Validation accuracy: 0.6666666666666666\n","\n","======== Epoch 5 / 10 ========\n","Training...\n","Total loss: 2.004633590579033\n","Validation accuracy: 0.6666666666666666\n","\n","======== Epoch 6 / 10 ========\n","Training...\n","Total loss: 1.249364659190178\n","Validation accuracy: 0.5\n","\n","======== Epoch 7 / 10 ========\n","Training...\n","Total loss: 0.557344876229763\n","Validation accuracy: 0.6666666666666666\n","\n","======== Epoch 8 / 10 ========\n","Training...\n","Total loss: 0.47249871492385864\n","Validation accuracy: 0.75\n","\n","======== Epoch 9 / 10 ========\n","Training...\n","Total loss: 0.13447163812816143\n","Validation accuracy: 0.6666666666666666\n","\n","======== Epoch 10 / 10 ========\n","Training...\n","Total loss: 0.09480103850364685\n","Validation accuracy: 0.6666666666666666\n","\n","Training complete!\n"]}]},{"cell_type":"markdown","metadata":{"id":"J9DpRJE5mHkO"},"source":["# Evaluate your model on the test set\n","After you're satisfied with your hyperparameters (i.e., you're unable to achieve higher validation accuracy by modifying them further), it's time to evaluate your model on the test set! Run the below cell to compute test set accuracy.\n"]},{"cell_type":"code","metadata":{"id":"msvZ78ii3cZZ","colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"status":"ok","timestamp":1636163364839,"user_tz":240,"elapsed":215,"user":{"displayName":"Shubham Shetty","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gift8m3G6SXjNLimI9-jKJTPDMNVC-Cto60CKMc=s64","userId":"02610804697838200174"}},"outputId":"182ed931-8f38-4133-eaf9-534fa4379d14"},"source":["get_validation_performance(test_set)"],"execution_count":111,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.8333333333333334"]},"metadata":{},"execution_count":111}]},{"cell_type":"markdown","metadata":{"id":"IcMT5aih8xEb"},"source":["## Question 2.1 (10 points):\n","Congratulations! You've now gone through the entire fine-tuning process and created a model for your downstream task. Two more questions left :) First, describe your hyperparameter selection process in words. If you based your process on any research papers or websites, please reference them. Why do you think the hyperparameters you ended up choosing worked better than others? Also, is there a significant discrepancy between your test and validation accuracy? Why do you think this is the case?\n","\n","### *WRITE YOUR ANSWER HERE*  \n","I used grid search for hyperparameter selection. Grid search is a straightforward way of selecting hyperparameters, where we go through a list of parameters exhaustively checking all combinations to find the best performing hyperparameters. It is kind of a brute-force approach to hyperparameter tuning.  \n","\n","I can't say why the chosen hyperparameters are better than the others I used during grid search. Hyperparam tuning is not an exact science, and it could be possible that there are better hyperparams for this model. A better approch for hyperparam tuning would be random search or Bayesian optimization.  \n","\n","There is a significant discrepancy between my test and validation accuracies. This is probably because the amount of training data is insufficient to generalise well, i.e. the model is unable to learn much from the limited amount of data provided. More training data would be needed to create a model with deep enough insight to generalise well on validation and test set."]},{"cell_type":"markdown","metadata":{"id":"NBbdMwt79fIs"},"source":["## Question 2.2 (20 points):\n","Finally, perform an *error analysis* on your model. This is good practice for your final project. Write some code in the below code cell to print out the text of up to five test set examples that your model gets **wrong** (if your model gets more than five test examples wrong, randomly choose five of them to print out). Then, in the following text cell, perform a qualitative analysis of these examples. See if you can figure out any reasons for errors that you observe, or if you have any informed guesses (e.g., common linguistic properties of these particular examples). Does this suggest any possible future steps could for your classifier?"]},{"cell_type":"code","metadata":{"id":"X72mumhI9WdR","colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"status":"ok","timestamp":1636163364841,"user_tz":240,"elapsed":21,"user":{"displayName":"Shubham Shetty","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gift8m3G6SXjNLimI9-jKJTPDMNVC-Cto60CKMc=s64","userId":"02610804697838200174"}},"outputId":"0e87f801-cc93-4d2d-b3bb-9858e495bac3"},"source":["## YOUR ERROR ANALYSIS CODE HERE\n","## print out up to 5 test set examples that your model gets wrong\n","# Put the model in evaluation mode\n","model.eval()\n","#batch_size=16\n","#print(len(test_set))\n","input_id_tensors = torch.stack([test[0] for test in test_set])\n","input_mask_tensors = torch.stack([test[1] for test in test_set])\n","label_tensors = torch.stack([test[2] for test in test_set])\n","\n","b_input_ids = input_id_tensors.to(device)\n","b_input_mask = input_mask_tensors.to(device)\n","b_labels = label_tensors.to(device)\n","\n","with torch.no_grad():        \n","    # Forward pass, calculate logit predictions.\n","    outputs = model(b_input_ids, \n","                            token_type_ids=None, \n","                            attention_mask=b_input_mask,\n","                            labels=b_labels)\n","    #print(outputs)\n","    logits = outputs.logits\n","    \n","    # Move logits and labels to CPU\n","    logits = logits.detach().cpu().numpy()\n","    label_ids = b_labels.to('cpu').numpy()\n","\n","    # Calculate the number of correctly labeled examples in batch\n","    pred_flat = np.argmax(logits, axis=1).flatten()\n","    labels_flat = label_ids.flatten()\n","    print(pred_flat)\n","    print(labels_flat)\n","\n","count = 1\n","for i in range(len(pred_flat)):\n","  if pred_flat[i] != labels_flat[i]:\n","    print(f\"Sentence {count}: {test_text[i]}\")\n","    count += 1\n","    if count > 5:\n","      break"],"execution_count":112,"outputs":[{"output_type":"stream","name":"stdout","text":["[1 0 1 0 1 0 0 0 1 0 0 1]\n","[1 0 0 1 1 0 0 0 1 0 0 1]\n","Sentence 1: @StellantisNA is poisoning a majority Black neighborhood because it doesn’t care. It only cares about profits. They planted a few trees and called it a day. What a disgrace. Residents warned this would happen &amp; City pushed it through.\n","Sentence 2: Nine months before Rosa Parks, 15-yr-old Claudette Colvin refused to give up her seat on a bus for a White woman. Now 82, she’s still on probation. @michele_norris\n"]}]},{"cell_type":"markdown","metadata":{"id":"6XyBdAup-e6Z"},"source":["### *DESCRIBE YOUR QUALITATIVE ANALYSIS OF THE ABOVE EXAMPLES HERE*"]},{"cell_type":"markdown","metadata":{"id":"pUZ8eOIECUBn"},"source":["The task of classifying sentences into genders is (as the annotators noted) extremely vague. Deeper analysis needs to be done to differentiate between male and female writing styles. This may be even more difficult to determine in the case of tweets, where the number of characters is restricetd.  \n","\n","Both sentences which are being mislabelled in the test set deal with the social issue of race discrimination. Obviously, race is an issue which has no gender, equally affecting both sexes. Which might be why the model would have a hard time classifying these tweets as male or female."]},{"cell_type":"markdown","metadata":{"id":"szIkBDiQ_Mkv"},"source":["\n","\n","---\n","\n","Finished? Remember to upload the PDF file of this notebook to Gradescope **AND** email your three dataset files (annotator1.csv, annotator2.csv, and final_data.csv) to cs685instructors@gmail.com with the subject line formatted as **Firstname_Lastname_HW1data**.\n"]}]}